{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a4e097",
   "metadata": {},
   "source": [
    "# Interactive Line Search Visualization\n",
    "\n",
    "This notebook provides **interactive visualizations** of line search methods for understanding optimization algorithms.\n",
    "\n",
    "## 📚 What You'll Learn\n",
    "\n",
    "1. How line search finds step sizes\n",
    "2. Comparison of different methods (Backtracking, Wolfe, Exact)\n",
    "3. Interactive parameter exploration\n",
    "4. Real-time convergence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aaf148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, FloatSlider, Dropdown\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec135128",
   "metadata": {},
   "source": [
    "## Part 1: Visualizing the φ(α) Function\n",
    "\n",
    "Let's visualize the one-dimensional line search problem:\n",
    "\n",
    "$$\\phi(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k)$$\n",
    "\n",
    "We'll use a simple quadratic example: $f(x,y) = x^2 + 4y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our test function and phi(alpha)\n",
    "def f(x, y):\n",
    "    \"\"\"Test function: f(x,y) = x² + 4y²\"\"\"\n",
    "    return x**2 + 4*y**2\n",
    "\n",
    "def grad_f(x, y):\n",
    "    \"\"\"Gradient of f\"\"\"\n",
    "    return np.array([2*x, 8*y])\n",
    "\n",
    "def phi(alpha, x_k, d_k):\n",
    "    \"\"\"Merit function: φ(α) = f(x_k + α·d_k)\"\"\"\n",
    "    x_new = x_k + alpha * d_k\n",
    "    return f(x_new[0], x_new[1])\n",
    "\n",
    "def phi_derivative(alpha, x_k, d_k):\n",
    "    \"\"\"Derivative: φ'(α) = ∇f(x_k + α·d_k)ᵀ d_k\"\"\"\n",
    "    x_new = x_k + alpha * d_k\n",
    "    grad = grad_f(x_new[0], x_new[1])\n",
    "    return np.dot(grad, d_k)\n",
    "\n",
    "print(\"✅ Functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c7fdd",
   "metadata": {},
   "source": [
    "### Interactive φ(α) Explorer\n",
    "\n",
    "**Try changing the starting point** and see how φ(α) changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    x0=FloatSlider(min=0.5, max=3.0, step=0.1, value=2.0, description='x₀'),\n",
    "    y0=FloatSlider(min=0.5, max=2.0, step=0.1, value=1.0, description='y₀'),\n",
    "    c=FloatSlider(min=0.01, max=0.5, step=0.01, value=0.1, description='c (Armijo)')\n",
    ")\n",
    "def plot_phi_interactive(x0=2.0, y0=1.0, c=0.1):\n",
    "    # Starting point and direction\n",
    "    x_k = np.array([x0, y0])\n",
    "    grad = grad_f(x_k[0], x_k[1])\n",
    "    d_k = -grad  # Steepest descent direction\n",
    "    \n",
    "    # Create phi(alpha) curve\n",
    "    alpha_range = np.linspace(0, 0.4, 1000)\n",
    "    phi_values = [phi(a, x_k, d_k) for a in alpha_range]\n",
    "    \n",
    "    # Find exact minimum\n",
    "    from scipy.optimize import minimize_scalar\n",
    "    result = minimize_scalar(lambda a: phi(a, x_k, d_k), bounds=(0, 1), method='bounded')\n",
    "    alpha_exact = result.x\n",
    "    phi_exact = result.fun\n",
    "    \n",
    "    # Armijo line\n",
    "    phi_0 = phi(0, x_k, d_k)\n",
    "    phi_prime_0 = phi_derivative(0, x_k, d_k)\n",
    "    armijo_line = phi_0 + c * alpha_range * phi_prime_0\n",
    "    \n",
    "    # Backtracking simulation\n",
    "    alpha_bt = 1.0\n",
    "    rho = 0.5\n",
    "    while phi(alpha_bt, x_k, d_k) > phi_0 + c * alpha_bt * phi_prime_0:\n",
    "        alpha_bt *= rho\n",
    "        if alpha_bt < 1e-10:\n",
    "            break\n",
    "    phi_bt = phi(alpha_bt, x_k, d_k)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left: phi(alpha)\n",
    "    ax1.plot(alpha_range, phi_values, 'b-', linewidth=2.5, label='φ(α) = f(xₖ + α·dₖ)')\n",
    "    ax1.plot(alpha_range, armijo_line, 'r--', linewidth=2, label=f'Armijo: φ(0) + {c}·α·φ\\'(0)')\n",
    "    ax1.plot(alpha_exact, phi_exact, 'go', markersize=12, label=f'Exact min (α={alpha_exact:.4f})', zorder=5)\n",
    "    ax1.plot(alpha_bt, phi_bt, 'rs', markersize=12, label=f'Backtracking (α={alpha_bt:.4f})', zorder=5)\n",
    "    ax1.axhline(y=phi_0, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax1.fill_between(alpha_range, 0, armijo_line, where=(np.array(phi_values) <= armijo_line), \n",
    "                      alpha=0.2, color='green', label='Acceptable region')\n",
    "    ax1.set_xlabel('Step size α', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('φ(α)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_title(f'Line Search from ({x0:.1f}, {y0:.1f})', fontsize=16, fontweight='bold')\n",
    "    ax1.legend(fontsize=11, loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, 0.4)\n",
    "    \n",
    "    # Right: 2D function with search direction\n",
    "    x = np.linspace(-0.5, 3.5, 100)\n",
    "    y = np.linspace(-0.5, 2.5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = X**2 + 4*Y**2\n",
    "    \n",
    "    contour = ax2.contour(X, Y, Z, levels=20, alpha=0.6, cmap='viridis')\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    \n",
    "    # Plot search line\n",
    "    alpha_line = np.linspace(0, 0.4, 50)\n",
    "    search_path = np.array([x_k + a * d_k for a in alpha_line])\n",
    "    ax2.plot(search_path[:, 0], search_path[:, 1], 'b-', linewidth=2, alpha=0.7, label='Search direction')\n",
    "    \n",
    "    # Mark points\n",
    "    ax2.plot(x_k[0], x_k[1], 'ko', markersize=12, label=f'Start xₖ=({x0:.1f},{y0:.1f})', zorder=5)\n",
    "    x_exact = x_k + alpha_exact * d_k\n",
    "    ax2.plot(x_exact[0], x_exact[1], 'go', markersize=12, label='Exact LS', zorder=5)\n",
    "    x_bt = x_k + alpha_bt * d_k\n",
    "    ax2.plot(x_bt[0], x_bt[1], 'rs', markersize=12, label='Backtracking', zorder=5)\n",
    "    ax2.plot(0, 0, 'r*', markersize=15, label='Global min (0,0)', zorder=5)\n",
    "    \n",
    "    # Gradient arrow\n",
    "    ax2.arrow(x_k[0], x_k[1], -grad[0]*0.15, -grad[1]*0.15, \n",
    "              head_width=0.15, head_length=0.1, fc='red', ec='red', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('x', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('y', fontsize=14, fontweight='bold')\n",
    "    ax2.set_title('Function Contours & Search Direction', fontsize=16, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting point: ({x0:.2f}, {y0:.2f}), f = {phi_0:.4f}\")\n",
    "    print(f\"Gradient: [{grad[0]:.2f}, {grad[1]:.2f}]\")\n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  Exact Line Search:  α = {alpha_exact:.6f}, φ(α) = {phi_exact:.6f}\")\n",
    "    print(f\"  Backtracking:       α = {alpha_bt:.6f}, φ(α) = {phi_bt:.6f}\")\n",
    "    print(f\"  Difference:         Δα = {abs(alpha_exact-alpha_bt):.6f}, Δφ = {abs(phi_exact-phi_bt):.6f}\")\n",
    "    print(f\"  Efficiency:         Backtracking ≈ {(phi_exact/phi_bt)*100:.1f}% as good as exact\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c70b3",
   "metadata": {},
   "source": [
    "## Part 2: Comparing Line Search Methods\n",
    "\n",
    "Let's implement and compare all three methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a02bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f_func, grad_func, x_k, d_k, alpha_init=1.0, rho=0.5, c=1e-4, max_iter=50):\n",
    "    \"\"\"Backtracking line search with Armijo condition\"\"\"\n",
    "    alpha = alpha_init\n",
    "    f_k = f_func(x_k[0], x_k[1])\n",
    "    grad_k = grad_func(x_k[0], x_k[1])\n",
    "    \n",
    "    grad_d = np.dot(grad_k, d_k)\n",
    "    evals = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        x_new = x_k + alpha * d_k\n",
    "        f_new = f_func(x_new[0], x_new[1])\n",
    "        evals += 1\n",
    "        \n",
    "        if f_new <= f_k + c * alpha * grad_d:\n",
    "            return alpha, evals\n",
    "        \n",
    "        alpha *= rho\n",
    "    \n",
    "    return alpha, evals\n",
    "\n",
    "def wolfe_line_search(f_func, grad_func, x_k, d_k, alpha_init=1.0, c1=1e-4, c2=0.9, max_iter=50):\n",
    "    \"\"\"Line search satisfying Wolfe conditions\"\"\"\n",
    "    alpha = alpha_init\n",
    "    f_k = f_func(x_k[0], x_k[1])\n",
    "    grad_k = grad_func(x_k[0], x_k[1])\n",
    "    grad_d_k = np.dot(grad_k, d_k)\n",
    "    evals = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        x_new = x_k + alpha * d_k\n",
    "        f_new = f_func(x_new[0], x_new[1])\n",
    "        grad_new = grad_func(x_new[0], x_new[1])\n",
    "        grad_d_new = np.dot(grad_new, d_k)\n",
    "        evals += 2  # Function + gradient\n",
    "        \n",
    "        # Check Armijo condition\n",
    "        if f_new > f_k + c1 * alpha * grad_d_k:\n",
    "            alpha *= 0.5\n",
    "            continue\n",
    "        \n",
    "        # Check curvature condition\n",
    "        if grad_d_new < c2 * grad_d_k:\n",
    "            alpha *= 1.5\n",
    "            continue\n",
    "        \n",
    "        return alpha, evals\n",
    "    \n",
    "    return alpha, evals\n",
    "\n",
    "def exact_line_search(f_func, x_k, d_k):\n",
    "    \"\"\"Exact line search (for comparison)\"\"\"\n",
    "    from scipy.optimize import minimize_scalar\n",
    "    result = minimize_scalar(lambda a: f_func((x_k + a * d_k)[0], (x_k + a * d_k)[1]), \n",
    "                            bounds=(0, 10), method='bounded')\n",
    "    return result.x, result.nfev\n",
    "\n",
    "print(\"✅ Line search methods implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e048e0",
   "metadata": {},
   "source": [
    "### Full Gradient Descent with Different Line Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f_func, grad_func, x0, method='backtracking', max_iter=50, tol=1e-6):\n",
    "    \"\"\"Gradient descent with specified line search method\"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    path = [x.copy()]\n",
    "    alphas = []\n",
    "    f_values = [f_func(x[0], x[1])]\n",
    "    total_evals = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_func(x[0], x[1])\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        d = -grad  # Steepest descent\n",
    "        \n",
    "        # Line search\n",
    "        if method == 'backtracking':\n",
    "            alpha, evals = backtracking_line_search(f_func, grad_func, x, d)\n",
    "        elif method == 'wolfe':\n",
    "            alpha, evals = wolfe_line_search(f_func, grad_func, x, d)\n",
    "        elif method == 'exact':\n",
    "            alpha, evals = exact_line_search(f_func, x, d)\n",
    "        else:\n",
    "            alpha, evals = 0.1, 1  # Fixed step\n",
    "        \n",
    "        total_evals += evals\n",
    "        alphas.append(alpha)\n",
    "        x = x + alpha * d\n",
    "        path.append(x.copy())\n",
    "        f_values.append(f_func(x[0], x[1]))\n",
    "    \n",
    "    return np.array(path), alphas, f_values, total_evals\n",
    "\n",
    "# Run all methods\n",
    "x0 = [2.0, 1.0]\n",
    "results = {}\n",
    "\n",
    "for method in ['backtracking', 'wolfe', 'exact']:\n",
    "    path, alphas, f_vals, evals = gradient_descent(f, grad_f, x0, method=method)\n",
    "    results[method] = {'path': path, 'alphas': alphas, 'f_vals': f_vals, 'evals': evals}\n",
    "    print(f\"{method.upper():15} | Iterations: {len(path)-1:3d} | Evaluations: {evals:4d} | Final f: {f_vals[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d3612",
   "metadata": {},
   "source": [
    "### Visual Comparison of All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# Contour plot\n",
    "x = np.linspace(-0.5, 2.5, 100)\n",
    "y = np.linspace(-0.5, 1.5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + 4*Y**2\n",
    "\n",
    "# Plot 1: Convergence paths\n",
    "ax = axes[0, 0]\n",
    "contour = ax.contour(X, Y, Z, levels=20, alpha=0.4, cmap='gray')\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "colors = {'backtracking': 'blue', 'wolfe': 'green', 'exact': 'red'}\n",
    "for method, data in results.items():\n",
    "    path = data['path']\n",
    "    ax.plot(path[:, 0], path[:, 1], 'o-', color=colors[method], \n",
    "            linewidth=2, markersize=6, label=f'{method.capitalize()} ({len(path)-1} iter)', alpha=0.7)\n",
    "\n",
    "ax.plot(x0[0], x0[1], 'ko', markersize=12, label='Start', zorder=5)\n",
    "ax.plot(0, 0, 'r*', markersize=15, label='Optimum', zorder=5)\n",
    "ax.set_xlabel('x', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('y', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Convergence Paths Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')\n",
    "\n",
    "# Plot 2: Function value convergence\n",
    "ax = axes[0, 1]\n",
    "for method, data in results.items():\n",
    "    ax.semilogy(data['f_vals'], 'o-', color=colors[method], linewidth=2, \n",
    "                markersize=5, label=method.capitalize())\n",
    "ax.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('f(x) [log scale]', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Function Value Convergence', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Step sizes\n",
    "ax = axes[1, 0]\n",
    "for method, data in results.items():\n",
    "    if data['alphas']:  # Check if not empty\n",
    "        ax.plot(data['alphas'], 'o-', color=colors[method], linewidth=2, \n",
    "                markersize=5, label=method.capitalize())\n",
    "ax.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Step size α', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Step Size Evolution', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Efficiency comparison (bar chart)\n",
    "ax = axes[1, 1]\n",
    "methods = list(results.keys())\n",
    "iterations = [len(results[m]['path'])-1 for m in methods]\n",
    "evaluations = [results[m]['evals'] for m in methods]\n",
    "final_f = [results[m]['f_vals'][-1] for m in methods]\n",
    "\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x_pos - width/2, iterations, width, label='Iterations', color='skyblue', edgecolor='black')\n",
    "ax.bar(x_pos + width/2, [e/10 for e in evaluations], width, label='Evaluations (÷10)', \n",
    "       color='lightcoral', edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Computational Cost Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([m.capitalize() for m in methods])\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for i, (it, ev) in enumerate(zip(iterations, evaluations)):\n",
    "    ax.text(i - width/2, it + 0.5, str(it), ha='center', fontweight='bold')\n",
    "    ax.text(i + width/2, ev/10 + 0.5, str(ev), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<15} {'Iterations':<12} {'Evaluations':<15} {'Final f(x)':<15}\")\n",
    "print(\"-\"*70)\n",
    "for method in methods:\n",
    "    data = results[method]\n",
    "    print(f\"{method.capitalize():<15} {len(data['path'])-1:<12} {data['evals']:<15} {data['f_vals'][-1]:<15.2e}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc2fb3",
   "metadata": {},
   "source": [
    "## Part 3: Parameter Sensitivity Analysis\n",
    "\n",
    "How do line search parameters affect convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    rho=FloatSlider(min=0.1, max=0.9, step=0.1, value=0.5, description='ρ (shrink)'),\n",
    "    c=FloatSlider(min=0.01, max=0.5, step=0.01, value=0.1, description='c (Armijo)')\n",
    ")\n",
    "def parameter_sensitivity(rho=0.5, c=0.1):\n",
    "    \"\"\"Explore how parameters affect backtracking performance\"\"\"\n",
    "    \n",
    "    def custom_backtracking(f_func, grad_func, x_k, d_k):\n",
    "        alpha = 1.0\n",
    "        f_k = f_func(x_k[0], x_k[1])\n",
    "        grad_k = grad_func(x_k[0], x_k[1])\n",
    "        grad_d = np.dot(grad_k, d_k)\n",
    "        evals = 0\n",
    "        \n",
    "        for i in range(50):\n",
    "            x_new = x_k + alpha * d_k\n",
    "            f_new = f_func(x_new[0], x_new[1])\n",
    "            evals += 1\n",
    "            \n",
    "            if f_new <= f_k + c * alpha * grad_d:\n",
    "                return alpha, evals\n",
    "            alpha *= rho\n",
    "        \n",
    "        return alpha, evals\n",
    "    \n",
    "    # Run gradient descent\n",
    "    x = np.array([2.0, 1.0], dtype=float)\n",
    "    path = [x.copy()]\n",
    "    alphas = []\n",
    "    total_evals = 0\n",
    "    \n",
    "    for i in range(50):\n",
    "        grad = grad_f(x[0], x[1])\n",
    "        if np.linalg.norm(grad) < 1e-6:\n",
    "            break\n",
    "        d = -grad\n",
    "        alpha, evals = custom_backtracking(f, grad_f, x, d)\n",
    "        alphas.append(alpha)\n",
    "        total_evals += evals\n",
    "        x = x + alpha * d\n",
    "        path.append(x.copy())\n",
    "    \n",
    "    path = np.array(path)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Convergence path\n",
    "    x_range = np.linspace(-0.5, 2.5, 100)\n",
    "    y_range = np.linspace(-0.5, 1.5, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = X**2 + 4*Y**2\n",
    "    \n",
    "    ax1.contour(X, Y, Z, levels=20, alpha=0.4, cmap='gray')\n",
    "    ax1.plot(path[:, 0], path[:, 1], 'bo-', linewidth=2, markersize=6)\n",
    "    ax1.plot(path[0, 0], path[0, 1], 'go', markersize=12, label='Start')\n",
    "    ax1.plot(0, 0, 'r*', markersize=15, label='Optimum')\n",
    "    ax1.set_title(f'Convergence Path\\nρ={rho:.1f}, c={c:.2f}', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('x', fontsize=12)\n",
    "    ax1.set_ylabel('y', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axis('equal')\n",
    "    \n",
    "    # Step sizes\n",
    "    ax2.plot(alphas, 'ro-', linewidth=2, markersize=6)\n",
    "    ax2.set_title(f'Step Sizes Over Iterations', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Iteration', fontsize=12)\n",
    "    ax2.set_ylabel('α', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nResults with ρ={rho:.2f}, c={c:.2f}:\")\n",
    "    print(f\"  Iterations: {len(path)-1}\")\n",
    "    print(f\"  Total function evaluations: {total_evals}\")\n",
    "    print(f\"  Final error: {np.linalg.norm(path[-1]):.2e}\")\n",
    "    print(f\"  Avg step size: {np.mean(alphas):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b74b7",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Backtracking** is simple and effective\n",
    "   - Low cost (1-3 evaluations per iteration)\n",
    "   - Good for most applications\n",
    "   - Easy to implement\n",
    "\n",
    "2. **Wolfe conditions** add curvature check\n",
    "   - Better for quasi-Newton methods\n",
    "   - Slightly more expensive\n",
    "   - Guarantees good convergence\n",
    "\n",
    "3. **Exact line search** is rarely worth it\n",
    "   - Minimal improvement over inexact\n",
    "   - Much higher computational cost\n",
    "   - Use only for special cases\n",
    "\n",
    "### Practical Recommendations:\n",
    "\n",
    "- **Start with backtracking** (ρ=0.5, c=0.1)\n",
    "- **Use Wolfe** for BFGS (c₁=1e-4, c₂=0.9)\n",
    "- **Avoid exact** unless function has special structure\n",
    "- **Tune parameters** based on your specific problem\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with the interactive widgets\n",
    "- Try different starting points\n",
    "- Test on more complex functions\n",
    "- Implement for your own optimization problems!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
