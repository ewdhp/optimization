# Layer 2: Core Optimization Methods

Welcome to the core of optimization theory! This layer covers fundamental optimization algorithms and theoretical results that form the basis of practical optimization.

## ğŸ¯ Learning Objectives

By completing this layer, you will:
- Master optimality conditions (FONC, SONC, SOSC)
- Implement gradient descent and variants
- Understand Newton's method and quasi-Newton approximations
- Learn trust region and line search strategies
- Master Lagrange multipliers and KKT conditions
- Understand duality theory and weak/strong duality
- Implement penalty and barrier methods

## ğŸ“š Module Structure

```
2-core-methods/
â”œâ”€â”€ unconstrained/           # Optimization without constraints
â”‚   â”œâ”€â”€ optimality-conditions/   # Necessary and sufficient conditions
â”‚   â”œâ”€â”€ gradient-methods/        # First-order methods
â”‚   â”œâ”€â”€ newton-methods/          # Second-order methods
â”‚   â””â”€â”€ momentum-methods/        # Accelerated methods
â”œâ”€â”€ constrained/             # Optimization with constraints
â”‚   â”œâ”€â”€ lagrange/               # Lagrange multiplier theory
â”‚   â”œâ”€â”€ kkt-conditions/         # KKT theory and applications
â”‚   â”œâ”€â”€ penalty-methods/        # Penalty and barrier approaches
â”‚   â””â”€â”€ active-set/             # Active set methods
â””â”€â”€ duality/                 # Duality theory
```

## ğŸ—ºï¸ Module Guide

### PART A: Unconstrained Optimization (Weeks 5-8)

#### Optimality Conditions (`unconstrained/optimality-conditions/`)

**Core Concepts:**
- **FONC** (First-Order Necessary): âˆ‡f(x*) = 0
- **SONC** (Second-Order Necessary): âˆ‡Â²f(x*) âª° 0
- **SOSC** (Second-Order Sufficient): âˆ‡Â²f(x*) â‰» 0
- Critical point classification: min/max/saddle

**Why It Matters:**
- FONC tells us where to look for optima (stationary points)
- SOSC guarantees a point is actually a minimum
- Understanding these is essential for algorithm design

**Files:**
- âœ… `first_order.py` - FONC, critical points, descent directions
- âœ… `second_order.py` - SONC, SOSC, Hessian analysis
- âœ… `examples.py` - Comprehensive worked examples

**Key Results:**
```
Minimum âŸ¹ âˆ‡f = 0 AND âˆ‡Â²f âª° 0  (necessary)
âˆ‡f = 0 AND âˆ‡Â²f â‰» 0 âŸ¹ Strict local minimum (sufficient)
```

#### Gradient Methods (`unconstrained/gradient-methods/`)

**Core Concepts:**
- Steepest descent: x_{k+1} = x_k - Î±_k âˆ‡f(x_k)
- Line search strategies: Armijo, Wolfe conditions
- Convergence analysis: rates, Lipschitz constants
- Conjugate gradient: improved search directions

**Why It Matters:**
- Simplest and most widely used optimization method
- Scales to large problems (only needs gradients)
- Foundation for advanced methods (Adam, RMSprop, etc.)

**Files:**
- âœ… `steepest_descent.py` - GD with various step sizes
- âœ… `line_search.py` - Armijo, Wolfe, strong Wolfe
- âœ… `convergence_analysis.py` - Lipschitz, strong convexity
- ğŸ”„ `conjugate_gradient.py` - CG method

**Convergence Rates:**
```
Convex smooth: O(1/k)
Strongly convex: O(Ïáµ) where Ï < 1 (linear convergence)
```

#### Newton Methods (`unconstrained/newton-methods/`)

**Core Concepts:**
- Newton's method: p_k = -[âˆ‡Â²f]^{-1}âˆ‡f
- Quasi-Newton: BFGS approximates Hessian
- Trust region: global convergence strategy
- Convergence theory: quadratic vs superlinear

**Why It Matters:**
- Fastest local convergence (quadratic)
- BFGS is workhorse of practical optimization
- Trust region ensures global convergence

**Files:**
- âœ… `newton_method.py` - Pure and damped Newton
- âœ… `quasi_newton.py` - BFGS, L-BFGS, DFP, SR1
- âœ… `trust_region.py` - TR with dogleg
- âœ… `convergence_theory.py` - Convergence proofs

**Convergence Rates:**
```
Newton: ||x_{k+1} - x*|| â‰¤ C||x_k - x*||Â² (quadratic)
BFGS: lim (||x_{k+1} - x*|| / ||x_k - x*||) = 0 (superlinear)
```

#### Momentum Methods (`unconstrained/momentum-methods/`)

**Core Concepts:**
- Heavy ball method: adds momentum term
- Nesterov acceleration: O(1/kÂ²) convergence
- Adaptive methods: AdaGrad, RMSprop, Adam

**Why It Matters:**
- Acceleration improves convergence significantly
- Adam is default for deep learning training
- Practical importance in large-scale problems

**Files:**
- ğŸ”„ `heavy_ball.py` - Classical momentum
- ğŸ”„ `nesterov_momentum.py` - Accelerated GD
- ğŸ”„ `adaptive_methods.py` - Adam, RMSprop

### PART B: Constrained Optimization (Weeks 9-12)

#### Lagrange Multipliers (`constrained/lagrange/`)

**Core Concepts:**
- Lagrangian: L(x, Î») = f(x) + Î£ Î»áµ¢háµ¢(x)
- Lagrange multiplier theorem
- Geometric interpretation: gradient alignment

**Why It Matters:**
- Fundamental theory for constrained optimization
- Converts constrained â†’ unconstrained stationarity
- Basis for KKT conditions

**Files:**
- âœ… `lagrange_multipliers.py` - Theory and examples
- ğŸ”„ `equality_constraints.py` - h(x) = 0 problems
- ğŸ”„ `examples.py` - Worked problems
- ğŸ”„ `geometric_interpretation.py` - Visual understanding

**Optimality Condition:**
```
âˆ‡f(x*) + Î£ Î»áµ¢* âˆ‡háµ¢(x*) = 0
háµ¢(x*) = 0  âˆ€i
```

#### KKT Conditions (`constrained/kkt-conditions/`)

**Core Concepts:**
- KKT conditions: stationarity, feasibility, complementarity
- Constraint qualification: LICQ, MFCQ, Slater
- Complementary slackness: Î»áµ¢gáµ¢(x) = 0

**Why It Matters:**
- Necessary conditions for constrained optimality
- Sufficient under convexity
- Basis for all constrained algorithms

**Files:**
- âœ… `kkt_theory.py` - Complete KKT theory
- ğŸ”„ `constraint_qualification.py` - CQ conditions
- ğŸ”„ `complementarity.py` - Complementary slackness
- ğŸ”„ `examples.py` - KKT examples

**KKT Conditions:**
```
1. Stationarity: âˆ‡f + Î£Î»áµ¢âˆ‡gáµ¢ + Î£Î¼â±¼âˆ‡hâ±¼ = 0
2. Primal feasibility: gáµ¢(x) ï¿½ï¿½ï¿½ 0, hâ±¼(x) = 0
3. Dual feasibility: Î»áµ¢ â‰¥ 0
4. Complementarity: Î»áµ¢gáµ¢(x) = 0
```

#### Penalty Methods (`constrained/penalty-methods/`)

**Core Concepts:**
- Exterior penalty: P(x, Ï) = f(x) + ÏÎ£[gáµ¢(x)â‚Š]Â²
- Interior penalty/barrier: B(x, Î¼) = f(x) - Î¼Î£log(-gáµ¢(x))
- Augmented Lagrangian: combines penalty + multipliers

**Why It Matters:**
- Convert constrained â†’ sequence of unconstrained
- Practical algorithms (IPOPT uses barrier)
- Foundation for interior point methods

**Files:**
- ğŸ”„ `penalty_functions.py` - Exterior penalty
- ğŸ”„ `barrier_methods.py` - Log barrier, interior point
- ğŸ”„ `augmented_lagrangian.py` - Method of multipliers

#### Active Set Methods (`constrained/active-set/`)

**Core Concepts:**
- Active constraints: those that are tight at solution
- Active set strategy for QP
- Sequential QP for nonlinear problems

**Why It Matters:**
- Identifies which constraints matter
- Efficient for problems with few active constraints
- SQP is powerful for nonlinear problems

**Files:**
- ğŸ”„ `active_set_method.py` - Active set for QP
- ğŸ”„ `qp_solver.py` - Quadratic programming solver
- ğŸ”„ `sequential_qp.py` - SQP algorithm

### PART C: Duality Theory (Weeks 13-16)

#### Duality (`duality/`)

**Core Concepts:**
- Lagrangian dual: d* = sup_Î» inf_x L(x, Î»)
- Weak duality: d* â‰¤ p* (always)
- Strong duality: d* = p* (under Slater)
- Saddle point interpretation

**Why It Matters:**
- Dual provides lower bound on optimal value
- Strong duality enables powerful solution methods
- Connects primal and dual problems

**Files:**
- âœ… `duality_theory.py` - Complete duality theory
- ğŸ”„ `dual_problems.py` - Constructing duals
- ğŸ”„ `saddle_points.py` - Saddle point characterization
- ğŸ”„ `minimax_theorem.py` - Von Neumann minimax

**Duality Gap:**
```
Gap = p* - d* â‰¥ 0
Gap = 0 âŸº strong duality holds
```

## ğŸ“– Suggested Study Order

### Weeks 5-6: Unconstrained Basics
1. **Optimality Conditions** (all 3 files)
2. **Gradient Methods** (steepest_descent, line_search)
3. Implement GD on test functions

### Weeks 7-8: Advanced Unconstrained
1. **Newton Methods** (newton, quasi_newton, trust_region)
2. **Convergence Theory**
3. Compare GD vs Newton vs BFGS on Rosenbrock

### Weeks 9-10: Basic Constrained
1. **Lagrange Multipliers**
2. **KKT Conditions**
3. Solve constrained problems analytically

### Weeks 11-12: Constrained Algorithms
1. **Penalty Methods**
2. **Active Set Methods**
3. Implement penalty method

### Weeks 13-16: Duality
1. **Duality Theory**
2. **Dual Problems**
3. Verify strong duality on examples

## ğŸ”— Connections to Layer 3

Layer 2 provides the foundation for specialized methods in Layer 3:

- **Gradient methods â†’ Stochastic optimization**: SGD extends GD
- **Newton â†’ Interior point**: Barrier methods for LP/SDP
- **KKT â†’ Linear programming**: Simplex uses complementarity
- **Duality â†’ Convex programming**: Strong duality in CVX
- **Active set â†’ Integer programming**: Branch & bound

## ğŸ“ Assessment

Before moving to Layer 3, you should be able to:

### Unconstrained
1. âœ… Verify FONC, SONC, SOSC at a point
2. âœ… Implement gradient descent from scratch
3. âœ… Implement BFGS algorithm
4. âœ… Explain convergence rates (linear, quadratic, superlinear)
5. âœ… Choose appropriate method for a problem

### Constrained
1. âœ… Write down KKT conditions for a problem
2. âœ… Verify KKT conditions at a candidate point
3. âœ… Construct Lagrangian and dual problem
4. âœ… Check constraint qualification
5. âœ… Implement penalty method

### Duality
1. âœ… Derive dual problem
2. âœ… Verify weak/strong duality
3. âœ… Interpret duality gap
4. âœ… Use dual to verify optimality

## ğŸ› ï¸ Practical Projects

### Project 1: Optimization Library
Build a Python library with:
- Gradient descent (multiple step sizes)
- Newton's method
- BFGS
- Penalty method

### Project 2: Rosenbrock Challenge
Minimize Rosenbrock function using:
- Steepest descent
- Newton
- BFGS
- Trust region
Compare iterations and function evaluations.

### Project 3: Constrained Portfolio
Solve portfolio optimization:
```
minimize    -r^T x + Î³x^T Î£ x
subject to  1^T x = 1, x â‰¥ 0
```
Using:
- KKT conditions (analytical)
- Penalty method (numerical)
- Compare solutions

## ğŸ“š Key References

### Textbooks
- **Nocedal & Wright**: "Numerical Optimization" (Chapters 2-18)
- **Boyd & Vandenberghe**: "Convex Optimization" (Chapters 4-5, 9-11)
- **Bertsekas**: "Nonlinear Programming" (Chapters 1-5)

### Papers
- **BFGS**: Broyden, Fletcher, Goldfarb, Shanno (1970)
- **Trust Region**: Conn, Gould, Toint (2000)
- **Interior Point**: Karmarkar (1984), Mehrotra (1992)

## ğŸ’¡ Implementation Tips

1. **Start Simple**: Implement basic GD before BFGS
2. **Test on Quadratics**: Quadratics have known solutions
3. **Visualize**: Plot convergence, contours, paths
4. **Use Test Functions**: Rosenbrock, Himmelblau, Beale
5. **Compare Methods**: See which works best for what

## ğŸ“Š Implementation Status

- **Unconstrained**: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80% (12/15 files)
- **Constrained**: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 14% (2/14 files)
- **Duality**: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 25% (1/4 files)
- **Overall**: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 46% (15/33 files)

## â­ï¸ Next Steps

Once you complete Layer 2, proceed to:
- **Layer 3**: Advanced topics (LP, NLP, IP, DP)
- Start with: `3-advanced-topics/linear-programming/`

---

**Status**: 15/33 modules complete (46%)
**Time Estimate**: 12 weeks of focused study
**Prerequisites**: Layer 1 (Foundations) complete
